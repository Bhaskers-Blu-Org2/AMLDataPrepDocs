{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/MachineLearningNotebooks/work-with-data/dataprep/how-to-guides/add-column-using-expression.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Column using Expression for Manipulating Stream Identifiers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Azure ML Data Prep you can add a new column to data with `Dataflow.add_column` by using a Data Prep expression to calculate the value from existing columns. For more examples of this refer to [add column using expression](./add-column-using-expression.ipynb).\n",
    "<p>\n",
    "Here we add additional columns based on the Path/Identifier of the Stream being read at the beggining of the Dataflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.dataprep as dprep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `RegEx.extract_record()`\n",
    "Using the `RegEx.extract_record()` expression, add a new record column \"Stream Date Record\", which contains the name capturing groups in the regex with value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dflow_regex_extract_record = dprep.auto_read_file('../data/stream-path.csv')\n",
    "regex = dprep.RegEx('\\/(?<year>\\d{4})\\/(?<month>\\d{2})\\/(?<day>\\d{2})\\/')\n",
    "dflow_regex_extract_record = dflow_regex_extract_record.add_column(new_column_name='Stream Date Record',\n",
    "                                                                   prior_column='Stream Path',\n",
    "                                                                   expression=regex.extract_record(dflow_regex_extract_record['Stream Path']))\n",
    "dflow_regex_extract_record.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `create_datetime()`\n",
    "Using the `create_datetime()` expression, add a new column \"Stream Date\", which contains datetime values constructed from year, month, day values extracted from a record column \"Stream Date Record\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = dprep.col('year', dflow_regex_extract_record['Stream Date Record'])\n",
    "month = dprep.col('month', dflow_regex_extract_record['Stream Date Record'])\n",
    "day = dprep.col('day', dflow_regex_extract_record['Stream Date Record'])\n",
    "dflow_create_datetime = dflow_regex_extract_record.add_column(new_column_name='Stream Date',\n",
    "                                                              prior_column='Stream Date Record',\n",
    "                                                              expression=dprep.create_datetime(year, month, day))\n",
    "dflow_create_datetime.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `create_http_stream_info()`\n",
    "Using the `create_http_stream_info()` expression, add a new column \"HttpStream\", which contains a Stream Info value constructed from a String value which represents a HTTP url."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dflow_urls = dprep.read_csv('../data/urls.csv')\n",
    "dflow_urls.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The urls in the 'Url' columns are currently just strings. They don't represent the data that could be read by doing a HTTP GET request on the url.\n",
    "\n",
    "In Data Prep binary streams are represented by Stream Info values. These values contain the path/identifier to access data at a location and the nature of that location (i.e. http address/local file/blob file).\n",
    "\n",
    "To create a HTTP Stream Info from a url in a column the `create_http_stream_info` function can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dflow_streaminfos = dflow_urls.add_column(dprep.create_http_stream_info(dflow_urls['Url']), 'HttpStream', 'Url')\n",
    "dflow_streaminfos.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the data at those urls can be read by renaming the column with Stream Infos ('HttpStream') to 'Path', then adding a `parse_*`/`read_*` step for the appropriate file format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dflow_url_paths = dflow_streaminfos.rename_columns({'HttpStream': 'Path'})\n",
    "dflow_url_read = dflow_url_paths.parse_delimited(separator=',', headers_mode=dprep.PromoteHeadersMode.CONSTANTGROUPED,\n",
    "    encoding=dprep.FileEncoding.UTF8, quoting=False, skip_rows=0, skip_mode=dprep.SkipMode.NONE, comment=None)\n",
    "dflow_url_read.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "sihhu"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "notice": "Copyright (c) Microsoft Corporation. All rights reserved. Licensed under the MIT License.",
  "test_has_service_dependencies": true
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
